{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://raw.githubusercontent.com/byteowlz/schemas/refs/heads/main/observational-memory/memory.schema.json",
  "title": "Observational Memory Configuration",
  "description": "Configuration for the pi observational-memory extension. Place as ~/.pi/agent/memory.json (global) or .pi/memory.json (project-local).",
  "type": "object",
  "properties": {
    "$schema": {
      "type": "string",
      "description": "JSON Schema reference for editor support"
    },
    "observer": {
      "type": "object",
      "description": "Observer agent settings. Compresses conversation messages into timestamped, prioritized observations.",
      "properties": {
        "provider": {
          "type": "string",
          "description": "LLM provider (e.g. openai, anthropic, google, ollama)",
          "default": "openai"
        },
        "model": {
          "type": "string",
          "description": "Model ID for the provider",
          "default": "gpt-5-nano"
        },
        "messageTokenThreshold": {
          "type": "integer",
          "description": "Token threshold for messages to trigger observation (estimated as chars / 4)",
          "default": 30000,
          "minimum": 1000
        },
        "temperature": {
          "type": "number",
          "description": "Sampling temperature for the observer LLM",
          "default": 0.3,
          "minimum": 0,
          "maximum": 2
        },
        "maxOutputTokens": {
          "type": "integer",
          "description": "Maximum output tokens for the observer response",
          "default": 100000,
          "minimum": 1000
        }
      },
      "additionalProperties": false
    },
    "reflector": {
      "type": "object",
      "description": "Reflector agent settings. Consolidates observations when they exceed the token threshold.",
      "properties": {
        "provider": {
          "type": "string",
          "description": "LLM provider (e.g. openai, anthropic, google, ollama)",
          "default": "openai"
        },
        "model": {
          "type": "string",
          "description": "Model ID for the provider",
          "default": "gpt-5-nano"
        },
        "observationTokenThreshold": {
          "type": "integer",
          "description": "Token count above which the reflector runs to consolidate observations",
          "default": 40000,
          "minimum": 5000
        },
        "temperature": {
          "type": "number",
          "description": "Sampling temperature for the reflector LLM (0 = deterministic)",
          "default": 0,
          "minimum": 0,
          "maximum": 2
        },
        "maxOutputTokens": {
          "type": "integer",
          "description": "Maximum output tokens for the reflector response",
          "default": 100000,
          "minimum": 1000
        }
      },
      "additionalProperties": false
    }
  },
  "additionalProperties": false
}
